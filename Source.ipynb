{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fe16cac0d50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bpemb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fe16cabdc90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bpemb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fe16cabdc50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bpemb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fe16cabdd50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bpemb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fe16cabd7d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bpemb/\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bpemb (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for bpemb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bpemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to dataset\n",
    "path_dataset = Path(\"D:\\\\Study\\\\Courses\\\\Web Science\\\\web_science_dataset.jsonl\")\n",
    "if not path_dataset.is_file():\n",
    "    print(\"File path incorrect!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset into a list\n",
    "dataset_list = []\n",
    "with open(path_dataset, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        dataset_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data from list of dicts to dataframe\n",
    "data_df = pd.DataFrame.from_records(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1066,)\n"
     ]
    }
   ],
   "source": [
    "data_df.head()\n",
    "print(data_df['question'].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the datafram to the csv\n",
    "data_df.to_csv(\"_dataset.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answer',\n",
       " 'answerId',\n",
       " 'answerUrl',\n",
       " 'category',\n",
       " 'categoryId',\n",
       " 'question',\n",
       " 'questionId',\n",
       " 'questionUrl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List the collumn headers\n",
    "list(data_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ans_mean      Ans_std  Category  Ques_mean   Ques_std\n",
      "0  2105.133838  2226.671978         0  55.603535  19.918188\n",
      "1  2624.635417  2039.914225         1  68.312500  22.117731\n",
      "2  2711.903614  2603.750532         2  56.656627  19.756565\n",
      "3  2025.311828  2244.153484         3  58.107527  19.494395\n",
      "4  2634.211712  2295.137713         4  58.081081  21.972192\n"
     ]
    }
   ],
   "source": [
    "#Compute data statistics\n",
    "def get_stats(x, Type):\n",
    "    sentence_length = [len(sentence) for sentence in x]\n",
    "    avg_length = np.mean(sentence_length)\n",
    "    std = np.std(sentence_length)\n",
    "    return {Type + \"_mean\":avg_length, Type + \"_std\":std}\n",
    "\n",
    "#Group data {Questions and Answers} by category\n",
    "grouped = data_df.groupby(['categoryId'])\n",
    "\n",
    "data_stats = []\n",
    "#Iterate over the groups\n",
    "for category,group in grouped:\n",
    "    \n",
    "    #Extract questions and answers\n",
    "    question_list = group['question'].values\n",
    "    answer_list = group['answer'].values\n",
    "    \n",
    "    #Get question and answer stats\n",
    "    row_dict = {}\n",
    "    row_dict.update(get_stats(question_list, \"Ques\"))\n",
    "    row_dict.update(get_stats(answer_list, \"Ans\"))\n",
    "    row_dict['Category'] = category\n",
    "    \n",
    "    #Save the stats\n",
    "    data_stats.append(row_dict)\n",
    "\n",
    "#Compute the statistics matrix\n",
    "df_statistics = pd.DataFrame.from_records(data_stats)\n",
    "print(df_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Vectorizer, convert the data into numeric vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Retrieve all questions\n",
    "ques_list = data_df['question'].values\n",
    "ques_id = data_df['categoryId'].values\n",
    "\n",
    "# Split dataset\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "ss.get_n_splits(ques_list, ques_id)\n",
    "\n",
    "X_train = y_train = y_test = X_test = None\n",
    "for train_index, test_index in ss.split(ques_list, ques_id):\n",
    "    X_train, X_test = ques_list[train_index], ques_list[test_index]\n",
    "    y_train, y_test = ques_id[train_index], ques_id[test_index]\n",
    "\n",
    "\n",
    "#Apply the vectorizer on the question list\n",
    "question_vectors = vectorizer.fit_transform(X_train)\n",
    "_test_question_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, we apply the Multinomial Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# classifier\n",
    "clf = MultinomialNB()\n",
    "    \n",
    "# Train the model\n",
    "clf.fit(question_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf_forest = RandomForestClassifier(n_estimators = 100)\n",
    "clf_forest.fit(question_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.6074766355140186\n",
      "Random Forest 0.5841121495327103\n"
     ]
    }
   ],
   "source": [
    "#Measure the accuracy\n",
    "print('Naive Bayes',clf.score(_test_question_vectors, y_test))\n",
    "print('Random Forest', clf_forest.score(_test_question_vectors, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Dataloader class\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define some globals\n",
    "batch_size = 10\n",
    "lr = 2e-4\n",
    "lstm_dim = 200\n",
    "n_epochs = 10\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_batch_bilstm(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Creates a tokenized batch for input to a bilstm model\n",
    "    :param text: A list of sentences to tokenize\n",
    "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
    "    :return: Tokenized text as well as the length of the input sequence\n",
    "    \"\"\"\n",
    "    # Some light preprocessing\n",
    "    input_ids = [tokenizer.encode_ids_with_eos(t) for t in text]\n",
    "    return input_ids, [len(ids) for ids in input_ids]\n",
    "\n",
    "def collate_batch_bilstm(input_data):\n",
    "    input_ids = [i[0][0] for i in input_data]\n",
    "    seq_lens = [i[1][0] for i in input_data]\n",
    "    labels = [i[2] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
    "\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    \n",
    "    \"\"\" Load the Sentence Dataset \"\"\"\n",
    "    def __init__(self, input_df, tokenizer):\n",
    "        \n",
    "        #Retrieve all questions and labels\n",
    "        self.questions = input_df['question'].values\n",
    "        self.labels = input_df['categoryId'].values\n",
    "        \n",
    "        #Store the dataset length\n",
    "        self.len = len(self.labels)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Select the corresponding index\n",
    "        question = self.questions[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Tokenise the question text\n",
    "        input_ids, seq_lens = text_to_batch_bilstm([question], self.tokenizer)\n",
    "        return input_ids, seq_lens, self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word embeddings\n",
    "from bpemb import BPEmb\n",
    "bpemb_en = BPEmb(lang='en', dim=300, vs=25000)\n",
    "\n",
    "#Add 0 embedding for padding\n",
    "pretrained_embeddings = np.concatenate([bpemb_en.emb.vectors, np.zeros(shape=(1,300))], axis=0)\n",
    "vocabulary = bpemb_en.emb.index2word + ['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_df, test_df = train_test_split(data_df, stratify=data_df['categoryId'].values, test_size=0.2)\n",
    "test_df, val_df = train_test_split(test_df, stratify=test_df['categoryId'].values, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train_dataset\n",
    "train_dataset = SentenceDataset(train_df, bpemb_en)\n",
    "train_dl = DataLoader(dataset=train_dataset, batch_size=10, shuffle=True, collate_fn=collate_batch_bilstm)\n",
    "\n",
    "# Load the validation_dataset\n",
    "val_dataset = SentenceDataset(val_df, bpemb_en)\n",
    "valid_dl = DataLoader(dataset=val_dataset, batch_size=10, shuffle=True, collate_fn=collate_batch_bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        super(BiLSTMNetwork, self).__init__()\n",
    "        \n",
    "        # Padding index is last in the array. Therefore we set padding_idx\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'bilstm': nn.LSTM(\n",
    "                pretrained_embeddings.shape[1],\n",
    "                lstm_dim,\n",
    "                2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'ff': nn.Linear(2*lstm_dim, n_classes)\n",
    "        })\n",
    "        self.n_classes = n_classes\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
    "                     list(self.model['ff'].named_parameters())\n",
    "        for n,p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        # Pack padded\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
    "        # Unpack (b x sl x 2*lstm_dim)\n",
    "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # Get the last output for classification (b x 2*lstm_dim)\n",
    "        ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
    "\n",
    "        # Get logits (b x 2)\n",
    "        logits = self.model['ff'](ff_in).view(-1, self.n_classes)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Xentropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = BiLSTMNetwork(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings), \n",
    "    lstm_dim=lstm_dim, \n",
    "    dropout_prob=0.1, \n",
    "    n_classes=len(set(data_df['categoryId'].values))\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "  return torch.sum(torch.argmax(logits, dim=-1) == labels).type(torch.float) / float(labels.shape[0])\n",
    "\n",
    "def train(model, train_dl, valid_dl, optimizer, n_epochs, device):\n",
    "  losses = []\n",
    "  best_acc = 0.0\n",
    "  for ep in range(n_epochs):\n",
    "    loss_epoch = []\n",
    "    for batch in tqdm(train_dl):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      seq_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      loss, logits = model(input_ids, seq_lens, labels=labels)\n",
    "      losses.append(loss.item())\n",
    "      loss_epoch.append(loss.item())\n",
    "      \n",
    "      loss.backward()\n",
    "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      optimizer.step()\n",
    "      #gc.collect()\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batch in valid_dl:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids = batch[0]\n",
    "        seq_lens = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        _, logits = model(input_ids, seq_lens, labels=labels)\n",
    "        acc = accuracy(logits, labels)\n",
    "        print(f'Validation accuracy: {acc}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
    "        best_model = model.state_dict()\n",
    "        if acc > best_acc:\n",
    "          #best_model = model.state_dict()\n",
    "          best_acc = acc\n",
    "        #gc.collect()\n",
    "\n",
    "  model.load_state_dict(best_model)\n",
    "  return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\dl\\lib\\site-packages\\ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767837eb7c684c96b8aa93873469c40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.6000000238418579, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.6000000238418579, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.800000011920929, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.6000000238418579, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.6000000238418579, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.5, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.699999988079071, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.800000011920929, train loss: 1.372222630783569\n",
      "Validation accuracy: 0.699999988079071, train loss: 1.372222630783569\n",
      "Validation accuracy: 1.0, train loss: 1.372222630783569\n",
      "Validation accuracy: 1.0, train loss: 1.372222630783569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3496ecd409214d81982a6c1044d0d618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.8419690236102703\n",
      "Validation accuracy: 1.0, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.8419690236102703\n",
      "Validation accuracy: 0.5714285969734192, train loss: 0.8419690236102703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c91b651037d4bf9b352f3e6ea2a8fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.4000000059604645, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.4000000059604645, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.6412608724000842\n",
      "Validation accuracy: 0.8571429252624512, train loss: 0.6412608724000842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2b6bd9ce2343bbb9d86315a784610a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.5042707013875939\n",
      "Validation accuracy: 1.0, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.5042707013875939\n",
      "Validation accuracy: 0.8571429252624512, train loss: 0.5042707013875939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e986dff00b504e4c8ffa3be4acb0b490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.35454053197835766\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.35454053197835766\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.35454053197835766\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.35454053197835766\n",
      "Validation accuracy: 1.0, train loss: 0.35454053197835766\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.35454053197835766\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.35454053197835766\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.35454053197835766\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.35454053197835766\n",
      "Validation accuracy: 1.0, train loss: 0.35454053197835766\n",
      "Validation accuracy: 1.0, train loss: 0.35454053197835766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ccf563f49446a182ff125ab6e0386c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.287609976044921\n",
      "Validation accuracy: 0.7142857313156128, train loss: 0.287609976044921\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3692ba69c94099845063226a32b8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.26584671883908817\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.26584671883908817\n",
      "Validation accuracy: 1.0, train loss: 0.26584671883908817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190ba1e33cf94c62a78d50e3a90a656f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.5, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.2128317209697047\n",
      "Validation accuracy: 0.7142857313156128, train loss: 0.2128317209697047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db281efe7afe4bf48961575cbbd0d060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.15766642623862556\n",
      "Validation accuracy: 1.0, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.6000000238418579, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.15766642623862556\n",
      "Validation accuracy: 0.8571429252624512, train loss: 0.15766642623862556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7425c81cdb4b41cea8ff80d90128cffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.9000000357627869, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.699999988079071, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.800000011920929, train loss: 0.12897106530253105\n",
      "Validation accuracy: 0.4285714626312256, train loss: 0.12897106530253105\n"
     ]
    }
   ],
   "source": [
    "# Create the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
